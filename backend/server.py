#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MediaCrawler Dashboard Backend
FastAPI server with WebSocket support for real-time crawling monitoring
"""

import asyncio
import json
import sys
import os
import time
from typing import Dict, Any, Optional, List, Set
from contextlib import asynccontextmanager
from collections import deque  # å¼•å…¥åŒç«¯é˜Ÿåˆ—ç”¨äºŽé™åˆ¶æ—¥å¿—é•¿åº¦
from datetime import datetime
import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import redis.asyncio as redis

# Add parent directory to Python path to import MediaCrawler modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# from cmd_arg import ArgumentParser
from config import base_config
from base.base_crawler import AbstractCrawler
from media_platform.xhs import XiaoHongShuCrawler
from store import xhs as xhs_store
from var import crawler_type_var


class CrawlRequest(BaseModel):
    keyword: str
    count: int = 20
    platform: str = "xhs"
    session_id: Optional[str] = None  # å…è®¸å‰ç«¯æŒ‡å®šsession_id


class CrawlResponse(BaseModel):
    success: bool
    message: str
    session_id: Optional[str] = None


class SessionData:
    """ä¼šè¯æ•°æ®ï¼ŒçŠ¶æ€ä¸Žè¿žæŽ¥åˆ†ç¦»"""
    def __init__(self, max_logs=500):
        # ä½¿ç”¨ deque é™åˆ¶æœ€å¤§æ—¥å¿—æ¡æ•°ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
        self.logs: deque = deque(maxlen=max_logs)
        self.crawled_count: int = 0
        self.is_running: bool = False
        # æ”¯æŒåŒä¸€ä¼šè¯ä¸‹å¤šä¸ª WebSocket è¿žæŽ¥ï¼ˆå¤šæ ‡ç­¾é¡µç›‘æŽ§ï¼‰
        self.active_sockets: Set[WebSocket] = set()
        self.start_time: Optional[float] = None
        self.end_time: Optional[float] = None
        self.error_message: Optional[str] = None
        # è®°å½•ä¼šè¯å…³è”çš„è¯·æ±‚ä¿¡æ¯ï¼Œç”¨äºŽåŽ†å²æŸ¥è¯¢
        self.keyword: str = ""
        self.platform: str = ""
        self.request_count: int = 0


class SessionManager:
    """ä¼˜åŒ–çš„ä¼šè¯ç®¡ç†å™¨ï¼šçŠ¶æ€ä¸Žè¿žæŽ¥åˆ†ç¦»"""
    def __init__(self):
        self.sessions: Dict[str, SessionData] = {}
        self.redis_client: Optional[redis.Redis] = None

    async def init_redis(self):
        """Initialize Redis connection for caching"""
        try:
            self.redis_client = redis.Redis(
                host='localhost',
                port=6379,
                decode_responses=True,
                socket_connect_timeout=5
            )
            await self.redis_client.ping()
            print("âœ… Redis connected successfully")
        except Exception as e:
            print(f"âš ï¸ Redis connection failed: {e}")
            self.redis_client = None

    def get_session(self, session_id: str) -> SessionData:
        """èŽ·å–æˆ–åˆ›å»ºä¼šè¯"""
        if session_id not in self.sessions:
            self.sessions[session_id] = SessionData()
            print(f"ðŸ†• Created new session: {session_id}")
        return self.sessions[session_id]

    async def connect(self, session_id: str, websocket: WebSocket):
        """WebSocketè¿žæŽ¥ - æ–­ç‚¹ç»­ä¼ """
        await websocket.accept()
        session = self.get_session(session_id)
        session.active_sockets.add(websocket)
        print(f"ðŸ”Œ WebSocket connected for session: {session_id}")

        # 1. ç«‹å³å‘é€æœ€è¿‘çš„åŽ†å²æ—¥å¿— (æ–­ç‚¹ç»­ä¼ ä½“éªŒ)
        if session.logs:
            # åˆå¹¶å‘é€åŽ†å²æ—¥å¿—ä»¥å‡å°‘ç½‘ç»œå¼€é”€
            history_logs = "\n".join(session.logs)
            await websocket.send_text(history_logs)
            print(f"ðŸ“œ Sent {len(session.logs)} historical logs")

        # 2. å‘é€å½“å‰çŠ¶æ€
        await self.send_stat_update(session_id)
        print(f"ðŸ“Š Sent current status for session: {session_id}")

    def disconnect(self, session_id: str, websocket: WebSocket):
        """WebSocketæ–­å¼€ - ä¿ç•™çŠ¶æ€ï¼Œåªç§»é™¤å½“å‰è¿žæŽ¥"""
        if session_id in self.sessions:
            session = self.sessions[session_id]
            if websocket in session.active_sockets:
                session.active_sockets.discard(websocket)
            print(f"ðŸ”Œ WebSocket disconnected for session: {session_id}")

    async def safe_emit(self, session_id: str, message: str):
        """å®‰å…¨å‘é€æ—¥å¿—æ–‡æœ¬"""
        session = self.get_session(session_id)
        session.logs.append(message)  # è‡ªåŠ¨ä¸¢å¼ƒæœ€æ—§çš„æ—¥å¿—

        # å‘æ‰€æœ‰æ´»è·ƒè¿žæŽ¥å¹¿æ’­æ—¥å¿—
        if session.active_sockets:
            dead_sockets = []
            for ws in list(session.active_sockets):
                try:
                    await ws.send_text(message)
                except Exception as e:
                    # è¿žæŽ¥å·²æ–­å¼€ï¼Œç§»é™¤å¤±æ•ˆçš„ socket
                    print(f"âš ï¸ Failed to send message to {session_id}: {e}")
                    dead_sockets.append(ws)
            for ws in dead_sockets:
                session.active_sockets.discard(ws)

    async def send_stat_update(self, session_id: str):
        """å‘é€ç»“æž„åŒ–ç»Ÿè®¡æ•°æ®"""
        session = self.get_session(session_id)
        if session.active_sockets:
            payload = json.dumps({
                "type": "stats",
                "crawled_count": session.crawled_count,
                "status": "running" if session.is_running else "stopped",
                "start_time": session.start_time,
                "error_message": session.error_message
            }, ensure_ascii=False)
            dead_sockets = []
            for ws in list(session.active_sockets):
                try:
                    await ws.send_text(payload)
                except Exception as e:
                    print(f"âš ï¸ Failed to send stats to {session_id}: {e}")
                    dead_sockets.append(ws)
            for ws in dead_sockets:
                session.active_sockets.discard(ws)

    async def send_data_update(self, session_id: str, data_item: dict):
        """å‘é€å®žæ—¶æ•°æ®æ›´æ–°"""
        session = self.get_session(session_id)
        if session.active_sockets:
            message = json.dumps({
                "type": "data",
                "data": data_item,
                "status": "success"
            }, ensure_ascii=False)
            dead_sockets = []
            for ws in list(session.active_sockets):
                try:
                    await ws.send_text(message)
                except Exception as e:
                    print(f"âš ï¸ Failed to send data to {session_id}: {e}")
                    dead_sockets.append(ws)
            for ws in dead_sockets:
                session.active_sockets.discard(ws)

    async def set_status(self, session_id: str, is_running: bool, error_message: str = None):
        """æ›´æ–°ä¼šè¯çŠ¶æ€"""
        session = self.get_session(session_id)
        session.is_running = is_running
        session.error_message = error_message

        if not is_running:
            session.end_time = asyncio.get_event_loop().time()

        await self.send_stat_update(session_id)

    async def increment_count(self, session_id: str):
        """å¢žåŠ çˆ¬å–è®¡æ•°"""
        session = self.get_session(session_id)
        session.crawled_count += 1
        await self.send_stat_update(session_id)

    # å…¼å®¹æ—§æŽ¥å£
    async def send_personal_message(self, message: Dict[str, Any], session_id: str):
        """å…¼å®¹æ—§çš„send_personal_messageæŽ¥å£"""
        if message.get("type") == "data":
            await self.send_data_update(session_id, message.get("data"))
        else:
            # å¯¹äºŽå…¶ä»–ç±»åž‹çš„æ¶ˆæ¯ï¼Œè½¬æ¢ä¸ºæ—¥å¿—æ ¼å¼
            msg_text = message.get("message", str(message))
            await self.safe_emit(session_id, msg_text)


# Global session manager
manager = SessionManager()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan"""
    await manager.init_redis()
    yield
    # Cleanup
    if manager.redis_client:
        await manager.redis_client.close()


app = FastAPI(
    title="MediaCrawler Dashboard API",
    description="Web UI for MediaCrawler with real-time monitoring",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite default port
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
async def root():
    return {"message": "MediaCrawler Dashboard API is running"}


@app.post("/api/crawl/start", response_model=CrawlResponse)
async def start_crawl(request: CrawlRequest, background_tasks: BackgroundTasks):
    """Start crawling process and return session ID"""
    try:
        # Generate or use provided session ID
        import uuid
        session_id = request.session_id or str(uuid.uuid4())

        # Update global configuration
        base_config.KEYWORDS = request.keyword
        base_config.CRAWLER_MAX_NOTES_COUNT = request.count
        base_config.PLATFORM = request.platform
        base_config.CRAWLER_TYPE = "search"

        # Try to create crawler instance first to test
        try:
            print(f"ðŸ“‹ Testing crawler creation for platform: {request.platform}")
            crawler_factory = CrawlerFactory()
            test_crawler = crawler_factory.create_crawler(request.platform)
            print(f"âœ… Test crawler created successfully: {type(test_crawler)}")
        except Exception as e:
            print(f"âŒ Failed to create crawler: {e}")
            raise HTTPException(status_code=500, detail=f"Crawler creation failed: {str(e)}")

        # ä½¿ç”¨ BackgroundTasks æäº¤ä»»åŠ¡ï¼ŒAPI ç«‹å³è¿”å›žæˆåŠŸ
        background_tasks.add_task(run_crawler_task, session_id, request)
        print(f"âœ… Crawler task scheduled for session: {session_id}")

        return CrawlResponse(
            success=True,
            message=f"Started crawling for '{request.keyword}' on {request.platform}",
            session_id=session_id
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start crawling: {str(e)}")


@app.websocket("/ws/logs/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """WebSocket endpoint for real-time log updates - æ”¯æŒæ–­ç‚¹ç»­ä¼ """
    await manager.connect(session_id, websocket)
    try:
        while True:
            # ä¿æŒè¿žæŽ¥æ´»è·ƒï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡ŒæŽ¥æ”¶å‰ç«¯çš„æŽ§åˆ¶æŒ‡ä»¤
            data = await websocket.receive_text()
            if data == "ping":
                await websocket.send_text("pong")
            elif data == "stop":
                # å¯ä»¥å¤„ç†åœæ­¢æŒ‡ä»¤
                await manager.set_status(session_id, False, "ç”¨æˆ·æ‰‹åŠ¨åœæ­¢")
    except WebSocketDisconnect:
        manager.disconnect(session_id, websocket)


@app.get("/api/sessions")
async def get_sessions_history(
    q: Optional[str] = None,
    date: Optional[str] = None,
    platform: Optional[str] = None,
):
    """
    èŽ·å–åŽ†å²ä¼šè¯åˆ—è¡¨ï¼Œæ”¯æŒå¤šç»´åº¦ç­›é€‰ï¼š
    - q: æŒ‰å…³é”®è¯æ¨¡ç³Šæœç´¢
    - date: æŒ‰æ—¥æœŸè¿‡æ»¤ï¼ˆYYYY-MM-DDï¼‰
    - platform: æŒ‰å¹³å°è¿‡æ»¤ï¼ˆæ”¯æŒ allï¼‰
    """
    summary_list: List[Dict[str, Any]] = []

    # è§£æžæ—¥æœŸç­›é€‰å‚æ•°
    target_date: Optional[datetime.date] = None
    if date:
        try:
            target_date = datetime.strptime(date, "%Y-%m-%d").date()
        except ValueError:
            target_date = None

    query = (q or "").strip().lower()

    for session_id, session in manager.sessions.items():
        keyword = getattr(session, "keyword", "") or ""
        platform_value = getattr(session, "platform", "") or ""
        start_time = getattr(session, "start_time", None)

        # 1. å…³é”®è¯æ¨¡ç³Šæœç´¢ï¼ˆä»…å¯¹ keywordï¼‰
        if query:
            if query not in keyword.lower():
                continue

        # 2. å¹³å°ç²¾ç¡®ç­›é€‰
        if platform and platform != "all" and platform_value != platform:
            continue

        # 3. æ—¥æœŸç­›é€‰ï¼ˆåŸºäºŽ start_time æ—¶é—´æˆ³ï¼‰
        if target_date:
            if not start_time:
                continue
            session_date = datetime.fromtimestamp(start_time).date()
            if session_date != target_date:
                continue

        summary_list.append(
            {
                "session_id": session_id,
                "keyword": keyword,
                "platform": platform_value,
                "start_time": start_time,
                "crawled_count": session.crawled_count,
                "status": "running" if session.is_running else "stopped",
            }
        )

    # æŒ‰æ—¶é—´å€’åºæŽ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨æœ€ä¸Šé¢ï¼‰
    summary_list.sort(key=lambda item: item.get("start_time") or 0, reverse=True)

    return {"total": len(summary_list), "sessions": summary_list}


async def run_crawler_task(session_id: str, request: CrawlRequest):
    """ä¼˜åŒ–ç‰ˆçˆ¬è™«ä»»åŠ¡ï¼šçŠ¶æ€ä¸Žè¿žæŽ¥åˆ†ç¦»"""
    session = manager.get_session(session_id)

    if session.is_running:
        await manager.safe_emit(session_id, "âš ï¸ ä»»åŠ¡å·²ç»åœ¨è¿è¡Œä¸­ï¼Œè¯·å‹¿é‡å¤å¯åŠ¨ã€‚")
        return

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    session.is_running = True
    session.crawled_count = 0
    # ä½¿ç”¨çœŸå®žæ—¶é—´æˆ³ï¼Œä¾¿äºŽå‰ç«¯å±•ç¤º
    session.start_time = time.time()
    session.error_message = None
    session.logs.clear()  # æ–°ä»»åŠ¡å¼€å§‹æ¸…ç©ºæ—§æ—¥å¿—
    # ä¿å­˜è¯·æ±‚å…ƒæ•°æ®ï¼Œä¾›åŽ†å²æŽ¥å£ä½¿ç”¨
    session.keyword = request.keyword
    session.platform = request.platform
    session.request_count = request.count

    await manager.safe_emit(session_id, f"ðŸš€ ä»»åŠ¡å¯åŠ¨: {request.platform} - {request.keyword}")

    # æ ¸å¿ƒå›žè°ƒå‡½æ•° - æ³¨å…¥åˆ°çˆ¬è™«å†…æ ¸
    async def on_crawler_update(message: str, data_item: dict = None):
        """
        çˆ¬è™«æ›´æ–°å›žè°ƒå‡½æ•°
        message: æ—¥å¿—æ–‡æœ¬
        data_item: å¦‚æžœçˆ¬åˆ°äº†æ•°æ®ï¼Œä¼ è¿›æ¥ï¼Œç”¨äºŽè®¡æ•°
        """
        # 1. å¤„ç†æ—¥å¿—
        if message:
            await manager.safe_emit(session_id, message)

        # 2. å¤„ç†è®¡æ•° (å¦‚æžœä¼ äº† data_item æˆ–è€…æ£€æµ‹åˆ°ç‰¹å®šå…³é”®è¯)
        if data_item or "ä¿å­˜æˆåŠŸ" in message:
            await manager.increment_count(session_id)

        # 3. å‘é€å®žæ—¶æ•°æ®æ›´æ–°
        if data_item:
            await manager.send_data_update(session_id, data_item)

    try:
        await manager.safe_emit(session_id, f"ðŸ” æ­£åœ¨åˆ›å»º {request.platform} çˆ¬è™«å®žä¾‹...")

        # åˆ›å»ºçˆ¬è™«å®žä¾‹
        crawler_factory = CrawlerFactory()
        crawler: AbstractCrawler = crawler_factory.create_crawler(request.platform)

        await manager.safe_emit(session_id, f"âœ… çˆ¬è™«å®žä¾‹åˆ›å»ºæˆåŠŸ: {type(crawler).__name__}")

        # å¦‚æžœæ˜¯å°çº¢ä¹¦çˆ¬è™«ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å›žè°ƒæ³¨å…¥æ–¹å¼
        if isinstance(crawler, XiaoHongShuCrawler):
            await manager.safe_emit(session_id, "ðŸ”§ é…ç½®å°çº¢ä¹¦çˆ¬è™«å®žæ—¶æ•°æ®æµ...")

            # ä¿å­˜åŽŸå§‹å‡½æ•°
            original_update_xhs_note = xhs_store.update_xhs_note

            # ä¼˜åŒ–çš„WebSocketæ•°æ®å‘é€å‡½æ•°
            async def update_xhs_note_with_websocket(note_item: Dict[str, Any]):
                # ç«‹å³å‘é€å®žæ—¶æ•°æ®
                await on_crawler_update(
                    f"ðŸ“ ç¬”è®° {note_item.get('note_id', 'N/A')} æ•°æ®èŽ·å–æˆåŠŸ",
                    data_item=note_item
                )

                # å¼‚æ­¥æ‰§è¡ŒåŽŸå§‹å­˜å‚¨æ“ä½œï¼Œä¸é˜»å¡žçˆ¬è™«
                try:
                    await original_update_xhs_note(note_item)
                except Exception as e:
                    await manager.safe_emit(session_id, f"âš ï¸ æ•°æ®å­˜å‚¨å¼‚å¸¸: {e}")

            # æ³¨å…¥å›žè°ƒå‡½æ•°
            xhs_store.update_xhs_note = update_xhs_note_with_websocket

            try:
                # è®¾ç½®çˆ¬è™«ç±»åž‹
                crawler_type_var.set("search")

                await manager.safe_emit(session_id, "ðŸš€ å¯åŠ¨æµè§ˆå™¨å’Œçˆ¬è™«ä»»åŠ¡...")
                await manager.set_status(session_id, True)

                # æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
                await asyncio.wait_for(crawler.start(), timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶

                await manager.safe_emit(session_id, "âœ… çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå®Œæˆ")

            except asyncio.TimeoutError:
                await manager.set_status(session_id, False, "ä»»åŠ¡è¶…æ—¶")
                await manager.safe_emit(session_id, "âŒ ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ï¼ˆ10åˆ†é’Ÿï¼‰")
            except Exception as e:
                await manager.set_status(session_id, False, str(e))
                await manager.safe_emit(session_id, f"âŒ çˆ¬è™«æ‰§è¡Œå¼‚å¸¸: {str(e)}")
                import traceback
                traceback.print_exc()
            finally:
                # æ¢å¤åŽŸå§‹å‡½æ•°
                xhs_store.update_xhs_note = original_update_xhs_note
                await manager.set_status(session_id, False)

        else:
            # å…¶ä»–å¹³å°çš„å¤„ç†é€»è¾‘
            await manager.safe_emit(session_id, f"ðŸ”„ å¯åŠ¨ {request.platform} å¹³å°çˆ¬è™«...")
            await manager.set_status(session_id, True)

            crawler_type_var.set("search")
            await crawler.start()

            await manager.set_status(session_id, False)
            await manager.safe_emit(session_id, f"âœ… {request.platform} çˆ¬è™«ä»»åŠ¡å®Œæˆ")

    except Exception as e:
        import traceback
        error_msg = f"ðŸ’¥ ä»»åŠ¡å¼‚å¸¸åœæ­¢: {str(e)}"
        print(traceback.format_exc())

        await manager.set_status(session_id, False, error_msg)
        await manager.safe_emit(session_id, error_msg)

    finally:
        await manager.safe_emit(session_id, "ðŸ ä»»åŠ¡ç»“æŸ")
        await manager.send_stat_update(session_id)


class CrawlerFactory:
    """Factory for creating platform-specific crawlers"""

    CRAWLERS = {
        "xhs": XiaoHongShuCrawler,
    }

    @staticmethod
    def create_crawler(platform: str) -> AbstractCrawler:
        crawler_class = CrawlerFactory.CRAWLERS.get(platform)
        if not crawler_class:
            raise ValueError(f"Unsupported platform: {platform}")
        return crawler_class()


@app.get("/api/crawl/history")
async def get_crawl_history():
    """Get crawling history from Redis cache"""
    try:
        if not manager.redis_client:
            return {"history": []}

        # Get last 10 crawl sessions
        sessions = await manager.redis_client.lrange("crawl_history", 0, 9)
        history = []

        for session in sessions:
            try:
                session_data = json.loads(session)
                history.append(session_data)
            except json.JSONDecodeError:
                continue

        return {"history": history}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get history: {str(e)}")


@app.get("/api/stats")
async def get_stats():
    """Get crawler statistics"""
    try:
        stats = {
            "active_connections": sum(len(session.active_sockets) for session in manager.sessions.values()),
            "total_sessions": len(manager.sessions),
            "platforms": list(CrawlerFactory.CRAWLERS.keys())
        }
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get stats: {str(e)}")


if __name__ == "__main__":
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=3450,
        reload=True,
        log_level="info"
    )
